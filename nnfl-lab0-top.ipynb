{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import all the necessary libraries and the functions"},{"metadata":{"id":"-NV4oC59AZaj","outputId":"50ca2086-b821-4540-80b3-886bdb96b358","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\nnp.random.seed(42)\ntf.random.set_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"id":"vCnxluwTA99o","outputId":"12abd138-ce87-4127-ae4b-41a529a32de5","trusted":true},"cell_type":"code","source":"XX = pd.read_csv(\"../input/nnfl-demo-lab-1/test-2.csv\")\ndf = pd.read_csv(\"../input/nnfl-demo-lab-1/train-2.csv\")\ndf.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the dataset provided"},{"metadata":{"id":"cL286sHu2flK","outputId":"f3868356-23e8-4c77-d828-f26b0942eafb","trusted":true},"cell_type":"code","source":"#correlation matrix\nk = 10\ncorrmat = df.corr()\ncols = corrmat.nlargest(k, 'Type')['Type'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eizi6gbv279I","outputId":"caf331b8-8043-4e7b-8646-0ef0288b73bb","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check mean and standard deviation of data to see if any data is very highly scattered"},{"metadata":{"id":"O198P3f8hlts","outputId":"5f0fd28e-d582-41b8-9fa8-4b396bfbae5c","trusted":true},"cell_type":"code","source":"df.groupby('Type').mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"VHkkPHn1n1Bb","outputId":"4c0c5793-fa5b-4132-8c42-6ea905bbf836","trusted":true},"cell_type":"code","source":"df.groupby('Type').std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now that we know which columns are actually good indicators of 'Type', we will take only those columns"},{"metadata":{"id":"nEB46qfsCaL2","trusted":true},"cell_type":"code","source":"X = df[['RI', 'Na', 'Mg', 'Al', 'Si', 'Ba']]\nXX = XX[['RI', 'Na', 'Mg', 'Al', 'Si', 'Ba']]\ny = df['Type']","execution_count":null,"outputs":[]},{"metadata":{"id":"R6UXaurEIOwC","outputId":"38bbe20f-efe3-4f3b-82f0-b4b6e10ba432","trusted":true},"cell_type":"code","source":"scalar = StandardScaler().fit(X)\nX_scaled = scalar.transform(X)\nXX_scaled = scalar.transform(XX)\n\nXX_scaled = pd.DataFrame(XX_scaled, columns = XX.columns, index=XX.index)\nX_scaled = pd.DataFrame(X_scaled, columns = X.columns, index=X.index)\nX_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating train and validation sets"},{"metadata":{"id":"ZhzJMx5bI-8I","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining and training the model."},{"metadata":{},"cell_type":"markdown","source":"Tried on 200 epochs, resulted in underfitting. Tried on 1000 epochs, gave overfitting, so settled for 500 epochs."},{"metadata":{"id":"clZjiYEMJu3p","outputId":"09af7c1c-452a-40e3-fcc5-908ec79c4533","trusted":true},"cell_type":"code","source":"tf.keras.backend.set_floatx('float64')\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(4, activation='relu'),\n    tf.keras.layers.Dense(8, activation='softmax')])\n\nmodel.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\nhistory = model.fit(x=X_train, y=y_train, batch_size = 18, validation_split=0.2, epochs = 500, verbose = 1)\nmodel.evaluate(X_test, y_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check to see if we are overfitting/underfitting"},{"metadata":{"id":"8XF_OmKHLEmj","outputId":"85fe7d17-4432-4570-c665-42cba2df5dc1","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.plot(epochs, val_loss, 'cyan', label='Validation Loss')\nplt.plot(epochs, val_acc, 'green', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Code to generate submission file and .h file"},{"metadata":{"id":"PzASxOpxRbZr","trusted":true},"cell_type":"code","source":"final_outputs = np.argmax(model.predict(XX_scaled), axis=-1)\nsample = pd.read_csv(\"../input/nnfl-demo-lab-1/sample_submission-2.csv\")\nsample.loc[:, \"Type\"] = final_outputs\nsample.to_csv(\"submission.csv\", index=False)\nmodel.save_weights('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Do upvote the notebook if you find it helpful! Thank you!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}